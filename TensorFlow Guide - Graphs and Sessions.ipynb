{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v\"1.10.0\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using TensorFlow\n",
    "tf_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs and Sessions\n",
    "\n",
    "TensorFlow uses a **dataflow graph** to represent your computation in terms of\n",
    "the dependencies between individual operations. This leads to a low-level\n",
    "programming model in which you first define the dataflow graph, then create a\n",
    "TensorFlow **session** to run parts of the graph across a set of local and\n",
    "remote devices.\n",
    "\n",
    "This guide will be most useful if you intend to use the low-level programming\n",
    "model directly. Higher-level APIs such as `estimator.Estimator` and Keras\n",
    "hide the details of graphs and sessions from the end user, but this guide may\n",
    "also be useful if you want to understand how these APIs are implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why dataflow graphs?\n",
    "\n",
    "![](https://www.tensorflow.org/images/tensors_flowing.gif)\n",
    "\n",
    "[Dataflow](https://en.wikipedia.org/wiki/Dataflow_programming) is a common\n",
    "programming model for parallel computing. In a dataflow graph, the nodes\n",
    "represent units of computation, and the edges represent the data consumed or\n",
    "produced by a computation. For example, in a TensorFlow graph, the matrix multiplication operation would correspond to a single node with two incoming edges (the\n",
    "matrices to be multiplied) and one outgoing edge (the result of the\n",
    "multiplication).\n",
    "\n",
    "Dataflow has several advantages that TensorFlow leverages when executing your\n",
    "programs:\n",
    "\n",
    "* **Parallelism.** By using explicit edges to represent dependencies between\n",
    "  operations, it is easy for the system to identify operations that can execute\n",
    "  in parallel.\n",
    "\n",
    "* **Distributed execution.** By using explicit edges to represent the values\n",
    "  that flow between operations, it is possible for TensorFlow to partition your\n",
    "  program across multiple devices (CPUs, GPUs, and TPUs) attached to different\n",
    "  machines. TensorFlow inserts the necessary communication and coordination\n",
    "  between devices.\n",
    "\n",
    "* **Compilation.** TensorFlow's [XLA compiler](https://www.tensorflow.org/performance/xla/) can\n",
    "  use the information in your dataflow graph to generate faster code, for\n",
    "  example, by fusing together adjacent operations.\n",
    "\n",
    "* **Portability.** The dataflow graph is a language-independent representation\n",
    "  of the code in your model. You can build a dataflow graph in Julia, store it\n",
    "  in a [SavedModel](http://malmaud.github.io/TensorFlow.jl/latest/saving.html), and restore it in a C++ program for\n",
    "  low-latency inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a `Graph`?\n",
    "\n",
    "A `Graph` contains two relevant kinds of information:\n",
    "\n",
    "* **Graph structure.** The nodes and edges of the graph, indicating how\n",
    "  individual operations are composed together, but not prescribing how they\n",
    "  should be used. The graph structure is like assembly code: inspecting it can\n",
    "  convey some useful information, but it does not contain all of the useful\n",
    "  context that source code conveys.\n",
    "\n",
    "* **Graph collections.** TensorFlow provides a general mechanism for storing\n",
    "  collections of metadata in a `Graph`. The `TensorFlow.add_to_collection` function\n",
    "  enables you to associate a list of objects with a key, and `get_collection` enables you to\n",
    "  look up all objects associated with a key. Many parts of the TensorFlow\n",
    "  library use this facility: for example, when you create a `Variable`, it\n",
    "  is added by default to collections representing \"variables\" and\n",
    "  \"trainable variables\". When you later come to create a `train.Saver` or\n",
    "  `train.Optimizer`, the variables in these collections are used as the\n",
    "  default arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a `Graph`\n",
    "\n",
    "Most TensorFlow programs start with a dataflow graph construction phase. In this\n",
    "phase, you invoke TensorFlow API functions that construct new `Operation`\n",
    "(node) and `Tensor` (edge) objects and add them to a `Graph`\n",
    "instance. TensorFlow provides a **default graph** that is an implicit argument\n",
    "to all API functions in the same context.  For example:\n",
    "\n",
    "* Calling `constant(42.0)` creates a single `Operation` that produces the\n",
    "  value `42.0`, adds it to the default graph, and returns a `Tensor` that\n",
    "  represents the value of the constant.\n",
    "\n",
    "* Calling `x * y` creates a single `Operation` that multiplies\n",
    "  the values of `Tensor` objects `x` and `y`, adds it to the default graph,\n",
    "  and returns a `Tensor` that represents the result of the multiplication.\n",
    "\n",
    "* Executing `v = Variable(0)` adds to the graph a `Operation` that will\n",
    "  store a writeable tensor value that persists between `run` calls.\n",
    "  The `Variable` object wraps this operation, and can be used [like a\n",
    "  tensor](#tensor-like_objects), which will read the current value of the\n",
    "  stored value. The `Variable` object also has methods such as\n",
    "  `assign` and `assign_add` that\n",
    "  create `Operation` objects that, when executed, update the stored value.\n",
    "  (See [Variables](./TensorFlow%20Guide%20-%20Variables.ipynb) for more information about variables.)\n",
    "\n",
    "* Calling `train.minimize` on a `train.Optimizer` will add operations and tensors to the\n",
    "  default graph that calculates gradients, and return a `Operation` that,\n",
    "  when run, will apply those gradients to a set of variables.\n",
    "\n",
    "Most programs rely solely on the default graph. However,\n",
    "see [Dealing with multiple graphs](#Programming-with-multiple-graphs) for more\n",
    "advanced use cases. High-level APIs such as the `estimator.Estimator` API\n",
    "manage the default graph on your behalf, and--for example--may create different\n",
    "graphs for training and evaluation.\n",
    "\n",
    "**⋆Note:** Calling most functions in the TensorFlow API merely adds operations\n",
    "and tensors to the default graph, but **does not** perform the actual\n",
    "computation. Instead, you compose these functions until you have a `Tensor`\n",
    "or `Operation` that represents the overall computation--such as performing\n",
    "one step of gradient descent--and then pass that object to a `Session` to\n",
    "perform the computation. See the section \"Executing a graph in a `Session`\"\n",
    "for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming operations\n",
    "\n",
    "A `Graph` object defines a **namespace** for the `Operation` objects it\n",
    "contains. TensorFlow automatically chooses a unique name for each operation in\n",
    "your graph, but giving operations descriptive names can make your program easier\n",
    "to read and debug. The TensorFlow API provides two ways to override the name of\n",
    "an operation:\n",
    "\n",
    "* Each API function that creates a new `Operation` or returns a new\n",
    "  `Tensor` accepts an optional `name` argument. For example,\n",
    "  `constant(42.0, name=\"answer\")` creates a new `Operation` named\n",
    "  `\"answer\"` and returns a `Tensor` named `\"answer:0\"`. If the default graph\n",
    "  already contains an operation named `\"answer\"`, then TensorFlow would append\n",
    "  `\"_1\"`, `\"_2\"`, and so on to the name, in order to make it unique.\n",
    "\n",
    "* **Name scopes**: not yet supported in TensorFlow.jl.\n",
    "\n",
    "Note that `Tensor` objects are implicitly named after the `Operation`\n",
    "that produces the tensor as output. A tensor name has the form `\"<OP_NAME>:<i>\"`\n",
    "where:\n",
    "\n",
    "* `\"<OP_NAME>\"` is the name of the operation that produces it.\n",
    "* `\"<i>\"` is an integer representing the index of that tensor among the\n",
    "  operation's outputs (follows Julia's 1-based indexing convention)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placing operations on different devices\n",
    "\n",
    "If you want your TensorFlow program to use multiple different devices, the\n",
    "`with_device` context manager function provides a convenient way to request that all operations\n",
    "created in a particular context are placed on the same device (or type of\n",
    "device).\n",
    "\n",
    "A **device specification** has the following form:\n",
    "\n",
    "```\n",
    "/job:<JOB_NAME>/task:<TASK_INDEX>/device:<DEVICE_TYPE>:<DEVICE_INDEX>\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "* `<JOB_NAME>` is an alpha-numeric string that does not start with a number.\n",
    "* `<DEVICE_TYPE>` is a registered device type (such as `GPU` or `CPU`).\n",
    "* `<TASK_INDEX>` is a positive integer representing the index of the task in the job named `<JOB_NAME>`.\n",
    "* `<DEVICE_INDEX>` is a positive integer representing the index of the\n",
    "  device, for example, to distinguish between different GPU devices used in the\n",
    "  same process.\n",
    "\n",
    "**⋆Note**: The indices used by TensorFlow.jl follows the 1-based Julia convention (e.g. \"gpu:1\" is the first GPU).\n",
    "\n",
    "You do not need to specify every part of a device specification. For example,\n",
    "if you are running in a single-machine configuration with a single GPU, you\n",
    "might use `with_device` to pin some operations to the CPU and GPU:\n",
    "\n",
    "```julia\n",
    "# Operations created outside either context will run on the \"best possible\"\n",
    "# device. For example, if you have a GPU and a CPU available, and the operation\n",
    "# has a GPU implementation, TensorFlow will choose the GPU.\n",
    "weights = random_normal(...)\n",
    "\n",
    "with_device(\"/device:CPU:1\") do\n",
    "    # Operations created in this context will be pinned to the CPU.\n",
    "    img = decode_jpeg(read_file(\"img.jpg\"))\n",
    "end\n",
    "\n",
    "with_device(\"/device:GPU:1\") do\n",
    "    # Operations created in this context will be pinned to the GPU.\n",
    "    result = weights * img\n",
    "end\n",
    "```\n",
    "\n",
    "**⋆Note**: distributed deployment of TensorFlow.jl not yet supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor-like objects\n",
    "\n",
    "Many TensorFlow operations take one or more `Tensor` objects as arguments.\n",
    "For example, `*` takes two `Tensor` objects, and `add_n` takes\n",
    "a list of `n` `Tensor` objects. For convenience, these functions will accept\n",
    "a **tensor-like object** in place of a `Tensor`, and implicitly convert it\n",
    "to a `Tensor` using the `convert` method. Tensor-like objects\n",
    "include elements of the following types:\n",
    "\n",
    "* `Tensor`\n",
    "* `Variable`\n",
    "* `Array`\n",
    "* Lists of tensor-like objects\n",
    "* Scalar Julia types: `Bool`, `Float64`, `Int64`, `String`\n",
    "\n",
    "You can register additional tensor-like types by extending the `Base.convert(::Type{Tensor}, value)` method.\n",
    "\n",
    "**⋆Note**: By default, TensorFlow will create a new `Tensor` each time you use\n",
    "the same tensor-like object. If the tensor-like object is large (e.g. an\n",
    "`Array` containing a set of training examples) and you use it multiple\n",
    "times, you may run out of memory. To avoid this, convert the tensor-like object to `Tensor` once and use the returned `Tensor` instead. Conversion can be done either by `convert(Tensor, value)` or `Tensor(value)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing a graph in a `Session`\n",
    "\n",
    "TensorFlow.jl uses the `Session` class to represent a connection between the\n",
    "client program---typically a Julia program, although a similar interface is\n",
    "available in other languages---and the C++ runtime. A `Session` object\n",
    "provides access to devices in the local machine, and remote devices using the\n",
    "distributed TensorFlow runtime. It also caches information about your\n",
    "`Graph` so that you can efficiently run the same computation multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a `Session`\n",
    "\n",
    "If you are using the low-level TensorFlow API, you can create a `Session`\n",
    "for the current default graph as follows:\n",
    "\n",
    "```julia\n",
    "# Create a default in-process session.\n",
    "sess = Session()\n",
    "# ...\n",
    "close(sess)\n",
    "\n",
    "# Create a remote session.\n",
    "sess = Session(\"grpc://example.org:2222\")\n",
    "# ...\n",
    "close(sess)\n",
    "```\n",
    "\n",
    "Since a `Session` owns physical resources (such as GPUs and network connections) and TensorFlow.jl does not yet have context manager support (Python `with` block) for sessions, you should remember to `close` the session when you are finished with it to free the resources.\n",
    "\n",
    "**⋆Note:** Higher-level APIs such as `train.MonitoredTrainingSession` or\n",
    "`estimator.Estimator` will create and manage a `Session` for you. These\n",
    "APIs accept optional `target` and `config` arguments (either directly, or as\n",
    "part of a `estimator.RunConfig` object), with the same meaning as\n",
    "described below.\n",
    "\n",
    "The `Session` constructor `Session(graph, config=nothing; target=nothin)` accepts three optional arguments:\n",
    "\n",
    "* **`graph`.** By default, a new `Session` will be bound to---and only able\n",
    "  to run operations in---the current default graph. If you are using multiple\n",
    "  graphs in your program (see [Programming with multiple\n",
    "  graphs](#Programming-with-multiple-graphs) for more details), you can specify\n",
    "  an explicit `Graph` when you construct the session.\n",
    "\n",
    "* **`config`.** This argument allows you to specify a `TensorFlow.tensorflow.ConfigProto` that\n",
    "  controls the behavior of the session. For example, some of the configuration\n",
    "  options include:\n",
    "\n",
    "    * `allow_soft_placement`. Set this to `true` to enable a \"soft\" device\n",
    "    placement algorithm, which ignores `device` annotations that attempt\n",
    "    to place CPU-only operations on a GPU device, and places them on the CPU\n",
    "    instead.\n",
    "\n",
    "    * `cluster_def` (not yet supported by TensorFlow.jl)\n",
    "\n",
    "    * `graph_options.optimizer_options`. Provides control over the optimizations\n",
    "    that TensorFlow performs on your graph before executing it.\n",
    "\n",
    "    * `gpu_options.allow_growth`. Set this to `True` to change the GPU memory\n",
    "    allocator so that it gradually increases the amount of memory allocated,\n",
    "    rather than allocating most of the memory at startup.\n",
    "\n",
    "* **`target`.** If this argument is left empty (the default), the session will\n",
    "  only use devices in the local machine. However, you may also specify a\n",
    "  `grpc://` URL to specify the address of a TensorFlow server, which gives the\n",
    "  session access to all devices on machines that this server controls. See\n",
    "  `train.Server` for details of how to create a TensorFlow\n",
    "  server. For example, in the common **between-graph replication**\n",
    "  configuration, the `Session` connects to a `train.Server` in the same\n",
    "  process as the client. The [distributed TensorFlow](https://www.tensorflow.org/deploy/distributed)\n",
    "  deployment guide describes other common scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `run` to execute operations\n",
    "\n",
    "The `run` method is the main mechanism for running a `Operation`\n",
    "or evaluating a `Tensor`. You can pass one or more `Operation` or\n",
    "`Tensor` objects to `run`, and TensorFlow will execute the\n",
    "operations that are needed to compute the result.\n",
    "\n",
    "`run` requires you to specify a list of **fetches**, which determine\n",
    "the return values, and may be a `Operation`, a `Tensor`, or\n",
    "a [tensor-like type](#Tensor-like-objects) such as `Variable`. These fetches\n",
    "determine what **subgraph** of the overall `Graph` must be executed to\n",
    "produce the result: this is the subgraph that contains all operations named in\n",
    "the fetch list, plus all operations whose outputs are used to compute the value\n",
    "of the fetches. For example, the following code fragment shows how different\n",
    "arguments to `run` cause different subgraphs to be executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output = "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-04 23:22:06.781998: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.18755e-8 1.0; 0.429209 0.570791]\n",
      "y_val = [7.22399 -8.49654; 1.51007 2.83216]\n",
      "output_val = [1.0 1.48819e-7; 0.21047 0.78953]\n"
     ]
    }
   ],
   "source": [
    "x = constant([37.0 -23.0; 1.0 4.0])\n",
    "w = random_uniform([2, 2], dtype=Float64)\n",
    "y = x * w\n",
    "output = nn.softmax(y)\n",
    "sess = Session()\n",
    "\n",
    "# Evaluate `output`. `run(sess, output)` will return an array containing the result of the computation.\n",
    "println(\"output = \", run(sess, output))\n",
    "\n",
    "# Evaluate `y` and `output`. Note that `y` will only be computed once, and its\n",
    "# result used both to return `y_val` and as an input to the `nn.softmax()`\n",
    "# op. Both `y_val` and `output_val` will be arrays.\n",
    "y_val, output_val = run(sess, [y, output])\n",
    "println(\"y_val = $y_val\")\n",
    "println(\"output_val = $output_val\")\n",
    "\n",
    "close(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run` also optionally takes a dictionary of **feeds**, which is a\n",
    "mapping from `Tensor` objects (typically `placeholder` tensors) to\n",
    "values (typically Julia scalars or arrays) that will be\n",
    "substituted for those tensors in the execution. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 4.0, 9.0]\n",
      "[0.0, 0.0, 25.0]\n"
     ]
    }
   ],
   "source": [
    "# Define a placeholder that expects a vector of three floating-point values,\n",
    "# and a computation that depends on it.\n",
    "x = placeholder(Float64, shape=[3])\n",
    "y = x .^ 2\n",
    "sess = Session()\n",
    "\n",
    "# Feeding a value changes the result that is returned when you evaluate `y`.\n",
    "println(run(sess, y, Dict(x => [1.0, 2.0, 3.0]))) # => \"[1.0, 4.0, 9.0]\"\n",
    "println(run(sess, y, Dict(x => [0.0, 0.0, 5.0]))) # => \"[0.0, 0.0, 25.0]\" \n",
    "\n",
    "# Raises an error, because you must feed a value for a `placeholder()` when\n",
    "# evaluating a tensor that depends on it.\n",
    "# run(sess, y)\n",
    "\n",
    "# Raises an error, because the shape of `37.0` does not match the shape\n",
    "# of placeholder `x`.\n",
    "# run(sess, y, Dict(x => 37.0))\n",
    "\n",
    "close(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**⋆Note**: optional arguments `options` and `run_metadata` for `run` not yet supported in TensorFlow.jl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing your graph\n",
    "\n",
    "TensorFlow includes tools that can help you to understand the code in a graph.\n",
    "The **graph visualizer** is a component of TensorBoard that renders the\n",
    "structure of your graph visually in a browser. The easiest way to create a\n",
    "visualization is to pass a `Graph` when creating the\n",
    "`TensorFlow.summary.FileWriter`:\n",
    "\n",
    "```julia\n",
    "# Build your graph.\n",
    "x = constant([37.0 -23.0; 1.0, 4.0])\n",
    "w = random_uniform([2, 2])\n",
    "y = x * w\n",
    "# ...\n",
    "loss = ...\n",
    "optimizer = train.AdamOptimizer()\n",
    "train_op = train.minimize(optimizer, loss)\n",
    "\n",
    "sess = Session()\n",
    "# `sess.graph` provides access to the graph used in a `Session`.\n",
    "writer = TensorFlow.summary.FileWriter(\"/tmp/log/...\"; graph=sess.graph)\n",
    "# Perform your computation...\n",
    "for i = 1:1000\n",
    "    run(sess, train_op)\n",
    "    # ...\n",
    "close(writer)\n",
    "```\n",
    "\n",
    "You can then open the log in `tensorboard`, navigate to the \"Graph\" tab, and\n",
    "see a high-level visualization of your graph's structure. Note that a typical\n",
    "TensorFlow graph---especially training graphs with automatically computed\n",
    "gradients---has too many nodes to visualize at once. The graph visualizer makes\n",
    "use of name scopes to group related operations into \"super\" nodes. You can\n",
    "click on the orange \"+\" button on any of these super nodes to expand the\n",
    "subgraph inside.\n",
    "\n",
    "![](https://www.tensorflow.org/images/mnist_deep.png)\n",
    "\n",
    "For more information about visualizing your TensorFlow application with\n",
    "TensorBoard, see the [TensorBoard guide](https://www.tensorflow.org/guide/summaries_and_tensorboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming with multiple graphs\n",
    "\n",
    "**⋆Note**: When training a model, a common way of organizing your code is to use one\n",
    "graph for training your model, and a separate graph for evaluating or performing\n",
    "inference with a trained model. In many cases, the inference graph will be\n",
    "different from the training graph: for example, techniques like dropout and\n",
    "batch normalization use different operations in each case. Furthermore, by\n",
    "default utilities like `train.Saver` use the names of `Variable` objects\n",
    "(which have names based on an underlying `Operation`) to identify each\n",
    "variable in a saved checkpoint. When programming this way, you can either use\n",
    "completely separate Julia processes to build and execute the graphs, or you can\n",
    "use multiple graphs in the same process. This section describes how to use\n",
    "multiple graphs in the same process.\n",
    "\n",
    "As noted above, TensorFlow provides a \"default graph\" that is implicitly passed\n",
    "to all API functions in the same context. For many applications, a single graph\n",
    "is sufficient. However, TensorFlow also provides methods for manipulating\n",
    "the default graph, which can be useful in more advanced use cases. For example:\n",
    "\n",
    "* A `Graph` defines the namespace for `Operation` objects: each\n",
    "  operation in a single graph must have a unique name. TensorFlow will\n",
    "  \"uniquify\" the names of operations by appending `\"_2\"`, `\"_3\"`, and so on to\n",
    "  their names if the requested name is already taken. Using multiple explicitly\n",
    "  created graphs gives you more control over what name is given to each\n",
    "  operation.\n",
    "  \n",
    "* The default graph stores information about every `Operation` and\n",
    "  `Tensor` that was ever added to it. If your program creates a large number\n",
    "  of unconnected subgraphs, it may be more efficient to use a different\n",
    "  `Graph` to build each subgraph, so that unrelated state can be garbage\n",
    "  collected.\n",
    "  \n",
    "You can install a different `Graph` as the default graph, using the\n",
    "`as_default` context manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_1 = Graph()\n",
    "as_default(g_1) do\n",
    "    # Operations created in this scope will be added to `g_1`.\n",
    "    global c = constant(\"Node in g_1\")\n",
    "    \n",
    "    # Sessions created in this scope will run operations from `g_1`.\n",
    "    global sess_1 = Session()\n",
    "end\n",
    "\n",
    "g_2 = Graph()\n",
    "as_default(g_2) do\n",
    "    # Operations created in this scope will be added to `g_2`.\n",
    "    global d = constant(\"Node in g_2\")\n",
    "end\n",
    "\n",
    "# Alternatively, you can pass a graph when constructing a `Session`:\n",
    "# `sess_2` will run operations from `g_2`.\n",
    "sess_2 = Session(graph=g_2)\n",
    "@assert get_graph(c) === g_1\n",
    "@assert sess_1.graph === g_1\n",
    "@assert get_graph(d) === g_2\n",
    "@assert sess_2.graph === g_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the current default graph, call `get_default_graph`, which\n",
    "returns a `Graph` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow.OperationIterator(Graph(Ptr{Nothing} @0x0000000005031550))\n"
     ]
    }
   ],
   "source": [
    "# Print all of the operations in the default graph.\n",
    "g = get_def_graph()\n",
    "println(get_operations(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "*Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). This notebook is adapted from the official [TensorFlow Guide on Graphs and Sessions](https://www.tensorflow.org/guide/graphs).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
